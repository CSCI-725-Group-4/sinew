\documentclass[sigconf]{acmart}


\begin{document}

\title{725 Progress Report}
\author{Marie Mellor \and Clinten Hopkins \and Danny Gardner}

\maketitle

\section{Accomplishments so far}
    %TODO

\section{Literature Report}
    % Kind of like the related works section of a paper 
    We focused on the comparisons between Sinew and the moden database services MongoDB, PostgreSQL JSON. Our goal in doing this is to see how Sinew compares to modern dataabse systems in aspects such as batch loading, querying, updating and overall performace and scalability. This section will cover the primary works used in our evaluation. 

    \subsection{Sinew}

    The main objective of Sinew\cite{Tahara_Diamond_Abadi_2014} is to enable developers to be able to represent data using self-describing formatting while still utilizing SQL and other traditional dataabse systems. It was built to be a middle layer between multi-structured data and a relational database management structure (RDBMS). Sinew acts as an efficient tool able to convert semi-structured data into something stored and maintainable in an RDBMS. 

    The Sinew system architecture is comprised of two layers, the use layer and the stroage layer. The storage layer allows Sinew to manage and query multi-structured data efficiently. Sinew's schema suppports universla relation in which one document corresponds to one row. For every key, value pair from a document, the value will logically get stored in the row and column whihc correspond to the document and key. A hybrid approach was used for mapping logial to physical schema to maintain performace benifits, space eficiency and single column serilization. The approach consisted of a combination of "all-physical-column" approach and "all-vitual-column" appraoch. Using this new hybrid appraich, columns for some of the attributes get created while the rest are stored in a special serilizated column called the column resivoir. This enables Sinew to maintain the benifits of using physical columns where they're needed while still keeping less frequenrly accessed keys tored virtually. 

    Sinew documents attribute names, types and storage menthods to maintain a correct mapping between logial and physcial and make system optimizations possible in a catalog. This catalog keeps track of which keys have been observed, the key type information which has been derived from data, the number of occurences of each key, if a column is virtual or physciual and a 'dirty' flag. The catalog is split into two parts so Sinew can easily identify logical schema and physical schema. 

    A scheme analyzer is also used to allow Sinew to adapt to evolvng data models and query patterns. This schema analyzer evaluates the current storage schema in order to determine a distribution of physcial and virtual columns. This was done to sminimize the overall cost of materialization while still maximizing increasing performace rates. 

    A column materializer was also used to maintain dynamic physical schema by moving data between the column resivoir and physcal columns. This materializer was created with the intent for it to be a background process that would run only when there are resouces available for it within tyhe system. This is where the "dirty column" as mentioned previously comes into play. A dirty column is where some values fro a key might exist in the resivoir when others exist in a corresponding physical column. These dirty columns make sure that dirty bits in a catalog are set for that specific column. 

    Within the use layer, data is loaded in two steps: serilization and insertion. During the serilization step, the loader first parses each document, making sure that its syntax is valid. After the validation, the document is serialized into the proper format. During the serilization step, the loader also aggregates information about the keys within the dataset such as presence, type and sparsity. This informaiton is then added to the catelog. During the insertion step, all the serilizated data is placed into the column resovoir and the dirty flag is set to true in the respective columns. The column data is then moved to physical columns once the dirty bit is picked up whihc creates the newly loaded data. This was put in place so the system components would stay modular. 

    Due to the nature of the hybrud storage, queries must match the physical schema. This is where a query rewreiter comes into play. Here, queries are converted into an abstract syntax tree so that they can be validated and later sent to the storage layer to be executed. If any of the column references cannot be validated, the physical column is rewritten. 


    \subsection{MongoDB}


    \subsection{PostgreSQL JSON}
    
    

\bibliographystyle{acm}
\bibliography{references}

\end{document}


