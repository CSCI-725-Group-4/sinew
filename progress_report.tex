\documentclass[sigconf]{acmart}


\begin{document}

\title{725 Progress Report}
\author{Marie Mellor \and Clinten Hopkins \and Danny Gardner}

\maketitle

\section{Accomplishments so far}
    %TODO

\section{Literature Report}
    % Kind of like the related works section of a paper 
    We focused on the comparisons between Sinew and the moden database services MongoDB, PostgreSQL JSON. Our goal in doing this is to see how Sinew compares to modern dataabse systems in aspects such as batch loading, querying, updating and overall performace and scalability. This section will cover the primary works used in our evaluation. 

    \subsection{Sinew}

    The main objective of Sinew\cite{Tahara_Diamond_Abadi_2014} is to enable developers to be able to represent data using self-describing formatting while still utilizing SQL and other traditional database systems. It was built to be a middle layer between multi-structured data and a relational database management structure (RDBMS). Sinew acts as an efficient tool able to convert semi-structured data into something stored and maintainable in an RDBMS. 

    The Sinew system architecture is comprised of two layers, the use layer and the storage layer. The storage layer allows Sinew to manage and query multi-structured data efficiently. Sinew's schema suppports universal relations in which one document corresponds to one row. For every key, value pair from a document, the value will logically get stored in the row and column which corresponds to the document and key. A hybrid approach was used for mapping logical to physical schema to maintain performace benefits, space efficiency and single column serialization. The approach consisted of a combination of "all-physical-column" approach and "all-virtual-column" appraoch. Using this new hybrid approach, columns for some of the attributes get created while the rest are stored in a special serialized column called the column reservoir. This enables Sinew to maintain the benefits of using physical columns where they're needed while still keeping less frequently accessed keys stored virtually. 

    Sinew documents attribute names, types and storage methods to maintain a correct mapping between logical and physical and make system optimizations possible in a catalog. This catalog keeps track of which keys have been observed, the key type information which has been derived from data, the number of occurences of each key, if a column is virtual or physical and a 'dirty' flag. The catalog is split into two parts so Sinew can easily identify logical schema and physical schema. 

    A scheme analyzer is also used to allow Sinew to adapt to evolvng data models and query patterns. This schema analyzer evaluates the current storage schema in order to determine a distribution of physical and virtual columns. This was done to minimize the overall cost of materialization while still maximizing increasing performace rates. 

    A column materializer was also used to maintain dynamic physical schema by moving data between the column reservoir and physical columns. This materializer was created with the intent for it to be a background process that would run only when there are resources available for it within the system. This is where the "dirty column" as mentioned previously comes into play. A dirty column is where some values for a key might exist in the reservoir when others exist in a corresponding physical column. These dirty columns make sure that dirty bits in a catalog are set for that specific column. 

    Within the use layer, data is loaded in two steps: serialization and insertion. During the serialization step, the loader first parses each document, making sure that its syntax is valid. After the validation, the document is serialized into the proper format. During the serialization step, the loader also aggregates information about the keys within the dataset such as presence, type and sparsity. This informaiton is then added to the catalog. During the insertion step, all the serializated data is placed into the column reservoir and the dirty flag is set to true in the respective columns. The column data is then moved to physical columns once the dirty bit is picked up which creates the newly loaded data. This was put in place so the system components would stay modular. 

    Due to the nature of the hybrid storage, queries must match the physical schema; this is where a query rewriter comes into play. Here, queries are converted into an abstract syntax tree so that they can be validated and later sent to the storage layer to be executed. If any of the column references cannot be validated, the physical column is rewritten. 


    \subsection{MongoDB}


    \subsection{PostgreSQL JSON}
    
    

\bibliographystyle{acm}
\bibliography{references}

\end{document}


